{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Try TextDataloader",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7fabc5fde5904b12a85562c26ea6810e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_211af0411bbc42df83d70819ca54d2fa",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_41d0f0a43330443eb147ad81bd35a148",
              "IPY_MODEL_6dfc8971b01f43abb0c3f9f82bf3f510"
            ]
          }
        },
        "211af0411bbc42df83d70819ca54d2fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "41d0f0a43330443eb147ad81bd35a148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_09ddfa1937ba4112a70f0de060aa48e7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_73b251ab121e45f8a5d2aafcfdd32120"
          }
        },
        "6dfc8971b01f43abb0c3f9f82bf3f510": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1fbbc5d33a8b470b8a584969495f2bba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 2.24MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_60bb3aba8dc54a87a7c547427a4fd035"
          }
        },
        "09ddfa1937ba4112a70f0de060aa48e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "73b251ab121e45f8a5d2aafcfdd32120": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1fbbc5d33a8b470b8a584969495f2bba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "60bb3aba8dc54a87a7c547427a4fd035": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9jAg3I25Kfr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "7c099e68-dab6-4317-b23a-06704bd9681a"
      },
      "source": [
        "try: import fastai2\n",
        "except: \n",
        "  !git clone https://github.com/richardyy1188/Pretrain-MLM-and-finetune-on-GLUE-with-fastai.git\n",
        "  %pip install -q fastai2 transformers tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Pretrain-MLM-and-finetune-on-GLUE-with-fastai'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/58)\u001b[K\rremote: Counting objects:   3% (2/58)\u001b[K\rremote: Counting objects:   5% (3/58)\u001b[K\rremote: Counting objects:   6% (4/58)\u001b[K\rremote: Counting objects:   8% (5/58)\u001b[K\rremote: Counting objects:  10% (6/58)\u001b[K\rremote: Counting objects:  12% (7/58)\u001b[K\rremote: Counting objects:  13% (8/58)\u001b[K\rremote: Counting objects:  15% (9/58)\u001b[K\rremote: Counting objects:  17% (10/58)\u001b[K\rremote: Counting objects:  18% (11/58)\u001b[K\rremote: Counting objects:  20% (12/58)\u001b[K\rremote: Counting objects:  22% (13/58)\u001b[K\rremote: Counting objects:  24% (14/58)\u001b[K\rremote: Counting objects:  25% (15/58)\u001b[K\rremote: Counting objects:  27% (16/58)\u001b[K\rremote: Counting objects:  29% (17/58)\u001b[K\rremote: Counting objects:  31% (18/58)\u001b[K\rremote: Counting objects:  32% (19/58)\u001b[K\rremote: Counting objects:  34% (20/58)\u001b[K\rremote: Counting objects:  36% (21/58)\u001b[K\rremote: Counting objects:  37% (22/58)\u001b[K\rremote: Counting objects:  39% (23/58)\u001b[K\rremote: Counting objects:  41% (24/58)\u001b[K\rremote: Counting objects:  43% (25/58)\u001b[K\rremote: Counting objects:  44% (26/58)\u001b[K\rremote: Counting objects:  46% (27/58)\u001b[K\rremote: Counting objects:  48% (28/58)\u001b[K\rremote: Counting objects:  50% (29/58)\u001b[K\rremote: Counting objects:  51% (30/58)\u001b[K\rremote: Counting objects:  53% (31/58)\u001b[K\rremote: Counting objects:  55% (32/58)\u001b[K\rremote: Counting objects:  56% (33/58)\u001b[K\rremote: Counting objects:  58% (34/58)\u001b[K\rremote: Counting objects:  60% (35/58)\u001b[K\rremote: Counting objects:  62% (36/58)\u001b[K\rremote: Counting objects:  63% (37/58)\u001b[K\rremote: Counting objects:  65% (38/58)\u001b[K\rremote: Counting objects:  67% (39/58)\u001b[K\rremote: Counting objects:  68% (40/58)\u001b[K\rremote: Counting objects:  70% (41/58)\u001b[K\rremote: Counting objects:  72% (42/58)\u001b[K\rremote: Counting objects:  74% (43/58)\u001b[K\rremote: Counting objects:  75% (44/58)\u001b[K\rremote: Counting objects:  77% (45/58)\u001b[K\rremote: Counting objects:  79% (46/58)\u001b[K\rremote: Counting objects:  81% (47/58)\u001b[K\rremote: Counting objects:  82% (48/58)\u001b[K\rremote: Counting objects:  84% (49/58)\u001b[K\rremote: Counting objects:  86% (50/58)\u001b[K\rremote: Counting objects:  87% (51/58)\u001b[K\rremote: Counting objects:  89% (52/58)\u001b[K\rremote: Counting objects:  91% (53/58)\u001b[K\rremote: Counting objects:  93% (54/58)\u001b[K\rremote: Counting objects:  94% (55/58)\u001b[K\rremote: Counting objects:  96% (56/58)\u001b[K\rremote: Counting objects:  98% (57/58)\u001b[K\rremote: Counting objects: 100% (58/58)\u001b[K\rremote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects:   2% (1/39)\u001b[K\rremote: Compressing objects:   5% (2/39)\u001b[K\rremote: Compressing objects:   7% (3/39)\u001b[K\rremote: Compressing objects:  10% (4/39)\u001b[K\rremote: Compressing objects:  12% (5/39)\u001b[K\rremote: Compressing objects:  15% (6/39)\u001b[K\rremote: Compressing objects:  17% (7/39)\u001b[K\rremote: Compressing objects:  20% (8/39)\u001b[K\rremote: Compressing objects:  23% (9/39)\u001b[K\rremote: Compressing objects:  25% (10/39)\u001b[K\rremote: Compressing objects:  28% (11/39)\u001b[K\rremote: Compressing objects:  30% (12/39)\u001b[K\rremote: Compressing objects:  33% (13/39)\u001b[K\rremote: Compressing objects:  35% (14/39)\u001b[K\rremote: Compressing objects:  38% (15/39)\u001b[K\rremote: Compressing objects:  41% (16/39)\u001b[K\rremote: Compressing objects:  43% (17/39)\u001b[K\rremote: Compressing objects:  46% (18/39)\u001b[K\rremote: Compressing objects:  48% (19/39)\u001b[K\rremote: Compressing objects:  51% (20/39)\u001b[K\rremote: Compressing objects:  53% (21/39)\u001b[K\rremote: Compressing objects:  56% (22/39)\u001b[K\rremote: Compressing objects:  58% (23/39)\u001b[K\rremote: Compressing objects:  61% (24/39)\u001b[K\rremote: Compressing objects:  64% (25/39)\u001b[K\rremote: Compressing objects:  66% (26/39)\u001b[K\rremote: Compressing objects:  69% (27/39)\u001b[K\rremote: Compressing objects:  71% (28/39)\u001b[K\rremote: Compressing objects:  74% (29/39)\u001b[K\rremote: Compressing objects:  76% (30/39)\u001b[K\rremote: Compressing objects:  79% (31/39)\u001b[K\rremote: Compressing objects:  82% (32/39)\u001b[K\rremote: Compressing objects:  84% (33/39)\u001b[K\rremote: Compressing objects:  87% (34/39)\u001b[K\rremote: Compressing objects:  89% (35/39)\u001b[K\rremote: Compressing objects:  92% (36/39)\u001b[K\rremote: Compressing objects:  94% (37/39)\u001b[K\rremote: Compressing objects:  97% (38/39)\u001b[K\rremote: Compressing objects: 100% (39/39)\u001b[K\rremote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "Unpacking objects:   1% (1/58)   \rUnpacking objects:   3% (2/58)   \rUnpacking objects:   5% (3/58)   \rUnpacking objects:   6% (4/58)   \rUnpacking objects:   8% (5/58)   \rUnpacking objects:  10% (6/58)   \rUnpacking objects:  12% (7/58)   \rUnpacking objects:  13% (8/58)   \rUnpacking objects:  15% (9/58)   \rUnpacking objects:  17% (10/58)   \rUnpacking objects:  18% (11/58)   \rUnpacking objects:  20% (12/58)   \rUnpacking objects:  22% (13/58)   \rUnpacking objects:  24% (14/58)   \rUnpacking objects:  25% (15/58)   \rUnpacking objects:  27% (16/58)   \rUnpacking objects:  29% (17/58)   \rUnpacking objects:  31% (18/58)   \rUnpacking objects:  32% (19/58)   \rUnpacking objects:  34% (20/58)   \rUnpacking objects:  36% (21/58)   \rUnpacking objects:  37% (22/58)   \rUnpacking objects:  39% (23/58)   \rUnpacking objects:  41% (24/58)   \rUnpacking objects:  43% (25/58)   \rUnpacking objects:  44% (26/58)   \rUnpacking objects:  46% (27/58)   \rUnpacking objects:  48% (28/58)   \rUnpacking objects:  50% (29/58)   \rUnpacking objects:  51% (30/58)   \rremote: Total 58 (delta 30), reused 42 (delta 17), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (58/58), done.\n",
            "\u001b[K     |████████████████████████████████| 194kB 2.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 665kB 8.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 16.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 35.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 43.0MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omRD3-nx5SVs",
        "colab_type": "code",
        "outputId": "0ff4a792-d0f3-40c1-df99-79345933618e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "7fabc5fde5904b12a85562c26ea6810e",
            "211af0411bbc42df83d70819ca54d2fa",
            "41d0f0a43330443eb147ad81bd35a148",
            "6dfc8971b01f43abb0c3f9f82bf3f510",
            "09ddfa1937ba4112a70f0de060aa48e7",
            "73b251ab121e45f8a5d2aafcfdd32120",
            "1fbbc5d33a8b470b8a584969495f2bba",
            "60bb3aba8dc54a87a7c547427a4fd035"
          ]
        }
      },
      "source": [
        "%cd Pretrain-MLM-and-finetune-on-GLUE-with-fastai\n",
        "\n",
        "from IPython.core.debugger import set_trace as bk\n",
        "from functools import partial\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from fastai2.text.all import *\n",
        "from transformers import ElectraTokenizer\n",
        "hf_tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-small-generator\")\n",
        "from _utils.hf_transformers_integration import HF_Tokenizer, HF_TextBlock, HFModelWrapper\n",
        "from _utils.demo_data import load_demo_dataframe"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Pretrain-MLM-and-finetune-on-GLUE-with-fastai\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fabc5fde5904b12a85562c26ea6810e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-SSKytH_6ns",
        "colab_type": "code",
        "outputId": "264052e0-541a-4fb1-beba-442bc1ae9f9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "source": [
        "# from the lyrics of \"Avicii - Waiting For Love\"\n",
        "data={'text':[\"Monday left me broken\",\"Tuesday I was through with hoping\",\"Wednesday my empty arms were open\",\"Thursday waiting for love, waiting for love\",\"Thank the stars it's Friday\",\"I'm burning like a fire gone wild on Saturday\",\"Guess I won't be coming to church on Sunday\",\"I'll be waiting for love, waiting for love\",\"To come around\",],'is_valid':[False]*7 + [True]*2}\n",
        "data['length'] = [ len(t.split()) for t in data['text']]\n",
        "df = pd.DataFrame(data=data)\n",
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>is_valid</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Monday left me broken</td>\n",
              "      <td>False</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tuesday I was through with hoping</td>\n",
              "      <td>False</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Wednesday my empty arms were open</td>\n",
              "      <td>False</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Thursday waiting for love, waiting for love</td>\n",
              "      <td>False</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thank the stars it's Friday</td>\n",
              "      <td>False</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>I'm burning like a fire gone wild on Saturday</td>\n",
              "      <td>False</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Guess I won't be coming to church on Sunday</td>\n",
              "      <td>False</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I'll be waiting for love, waiting for love</td>\n",
              "      <td>True</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>To come around</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            text  is_valid  length\n",
              "0                          Monday left me broken     False       4\n",
              "1              Tuesday I was through with hoping     False       6\n",
              "2              Wednesday my empty arms were open     False       6\n",
              "3    Thursday waiting for love, waiting for love     False       7\n",
              "4                    Thank the stars it's Friday     False       5\n",
              "5  I'm burning like a fire gone wild on Saturday     False       9\n",
              "6    Guess I won't be coming to church on Sunday     False       9\n",
              "7     I'll be waiting for love, waiting for love      True       8\n",
              "8                                 To come around      True       3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLiBRkiM5hgb",
        "colab_type": "text"
      },
      "source": [
        "#1. TextDataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xuPaHcu5jbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@delegates()\n",
        "class TextDataloader(TfmdDL):\n",
        "  def __init__(self, dataset, max_seq_len=float('inf'), sort_by_len='desc', agg_mode=None, ignore_gt_maxlen=True, remove_heads=False, remove_tails=False, bos_idx_add=None, eos_idx_add=None, show_bar=None, samples=None, **kwargs):\n",
        "    super().__init__(dataset, **kwargs)\n",
        "    assert agg_mode in [None, 'lm', 'lines', 'window']\n",
        "    assert not (agg_mode and max_seq_len is None)\n",
        "    assert sort_by_len in [False, 'desc', 'asc']\n",
        "    if agg_mode in ['window','lm']: sort_by_len=False # sorting makes no sense with these modes\n",
        "    ignore_gt_maxlen = ignore_gt_maxlen and agg_mode in [None, 'lines'] and max_seq_len is not None\n",
        "    first_text_tensor = dataset[0][0]\n",
        "    device, dtype = first_text_tensor.device, first_text_tensor.dtype\n",
        "    self.bos = torch.tensor([bos_idx_add] if bos_idx_add is not None else [], device=device, dtype=dtype)\n",
        "    self.eos = torch.tensor([eos_idx_add] if eos_idx_add is not None else [], device=device, dtype=dtype)\n",
        "    self.add_bos_or_eos = bos_idx_add or eos_idx_add\n",
        "    # only use [start:end] text to concatenate (if needed)\n",
        "    self.start = 0 if not remove_heads else 1\n",
        "    self.end = None if not remove_tails else -1\n",
        "    if show_bar is None: show_bar = len(dataset) > 200000\n",
        "\n",
        "    store_attr(self,'dataset,max_seq_len,sort_by_len,agg_mode,ignore_gt_maxlen,remove_heads,remove_tails,bos_idx_add,eos_idx_add,show_bar')\n",
        "    \n",
        "    if samples is not None: # Load from cache\n",
        "      if sort_by_len: self.samples = sorted(samples, key=lambda s: s[0], reverse=True if sort_by_len=='desc' else False)\n",
        "      else: self.samples = samples\n",
        "      self.n = len(samples)\n",
        "      return\n",
        "\n",
        "    self.samples = L()\n",
        "    # residual_len will reset to initial_residual_len\n",
        "    # lm mode: max_seq_len text and 1 right-shift text, so take max_seq_len + 1 window\n",
        "    self.initial_residual_len = max_seq_len + 1 if agg_mode=='lm' else max_seq_len \n",
        "    # keep spaces to add bos to final text \n",
        "    if bos_idx_add is not None: self.initial_residual_len -= 1\n",
        "    if eos_idx_add is not None: self.initial_residual_len -= 1\n",
        "    self.residual_len, self.new_sample = self.initial_residual_len, []\n",
        "\n",
        "    for i, sample in tqdm(enumerate(dataset), desc='TextDataloader init:', total=len(dataset), disable=not show_bar):\n",
        "      line_len = len(sample[0])\n",
        "      if remove_heads: line_len -= 1\n",
        "      if remove_tails: line_len -= 1\n",
        "      \n",
        "      if max_seq_len is not None and line_len > self.initial_residual_len and agg_mode in [None, 'lines']:\n",
        "        if ignore_gt_maxlen: continue\n",
        "        else: raise ValueError(f'The {i} th text line in dataset has length {line_len}(without removing head or tail, {len(sample[0])}), and is longer than max length {self.initial_residual_len}(without add bos or eos, {max_seq_len})')\n",
        "        \n",
        "      if agg_mode is None: self.samples.append( (line_len, i) )\n",
        "      elif agg_mode == 'lines': self._accumulate_lines(i, line_len)\n",
        "      else: self._accumulate_window(i, line_len)\n",
        "    \n",
        "    if agg_mode is not None and self.new_sample:\n",
        "      if agg_mode == 'lines': self.samples.append((self.max_seq_len-self.residual_len, self.new_sample))\n",
        "      else: self.samples.append(self.new_sample)\n",
        "\n",
        "    # sort if needed\n",
        "    if sort_by_len:\n",
        "      self.samples.sort(key=lambda s: s[0], reverse=True if sort_by_len=='desc' else False)\n",
        "    # specify total number of samples\n",
        "    self.n = len(self.samples)\n",
        "      \n",
        "  def _accumulate_lines(self, i, line_len):\n",
        "    if line_len <= self.residual_len:\n",
        "      self.new_sample.append(i)\n",
        "      self.residual_len -= line_len\n",
        "    else:\n",
        "      self.samples.append((self.max_seq_len-self.residual_len, self.new_sample))\n",
        "      self.new_sample = [i]\n",
        "      self.residual_len = self.initial_residual_len - line_len\n",
        "\n",
        "  def _accumulate_window(self, i, line_len):\n",
        "    usable_len = line_len\n",
        "    cursor = self.start\n",
        "    while usable_len != 0:\n",
        "      use_len = min(usable_len, self.residual_len)\n",
        "      self.new_sample.append((i, cursor, cursor+use_len))\n",
        "      self.residual_len -= use_len\n",
        "      usable_len -= use_len\n",
        "      cursor += use_len\n",
        "      if self.residual_len == 0:\n",
        "        self.samples.append(self.new_sample)\n",
        "        self.new_sample = []\n",
        "        self.residual_len = self.initial_residual_len\n",
        "\n",
        "  def create_item(self, s):\n",
        "    if self.agg_mode is None:\n",
        "      \"samples = [ (length, idx), ... ]\"\n",
        "      idx = self.samples[s][1]\n",
        "      sample = self.dataset[idx]\n",
        "      line = sample[0][self.start:self.end]\n",
        "      text = torch.cat([self.bos, line, self.eos]) if self.add_bos_or_eos else line\n",
        "      return ( TensorText(text), *sample[1:] )\n",
        "    elif self.agg_mode == 'lines':\n",
        "      \"samples = [ (length, [idx, idx, ...]) , ... ]\"\n",
        "      agg = [ self.dataset[idx][0][self.start:self.end] for idx in self.samples[s][1] ]\n",
        "      agg_text = concat(self.bos, *agg, self.eos) if self.add_bos_or_eos else concat(*agg)\n",
        "      return (TensorText(agg_text), )\n",
        "    else: # window or lm\n",
        "      \"samples = [ (idx,start,end) ]\"\n",
        "      agg = [ self.dataset[idx][0][start:end] for idx,start,end in self.samples[s] ]\n",
        "      agg_text = concat(self.bos, *agg, self.eos) if self.add_bos_or_eos else concat(*agg)\n",
        "      if self.agg_mode == 'window':\n",
        "        return (TensorText(agg_text), )\n",
        "      else: # 'lm'\n",
        "        return (LMTensorText(agg_text[:-1]), TensorText(agg_text[1:]))\n",
        "\n",
        "  def shuffle_fn(self, idxs):\n",
        "    if not self.sort_by_len: # notice sort_by_len in lm and winodw mode will be False\n",
        "      self.samples.shuffle()\n",
        "    return idxs\n",
        "\n",
        "  def desc_sort(self):\n",
        "    assert self.agg_mode not in ['window','lm'], f\"Sorting by length makes no sense on aggregation mode {self.agg_mode}\"\n",
        "    self.samples.sort(key=lambda s: s[0], reverse=True)\n",
        "    self.sort_by_len = 'desc'\n",
        "\n",
        "  def asc_sort(self):\n",
        "    assert self.agg_mode not in ['window','lm'], f\"Sorting by length makes no sense on aggregation mode {self.agg_mode}\"\n",
        "    self.samples.sort(key=lambda s: s[0], reverse=False)\n",
        "    self.sort_by_len = 'asc'\n",
        "\n",
        "  def cache(self, file_path):\n",
        "    torch.save(self, file_path)\n",
        "\n",
        "  def __getstate__(self):\n",
        "    \"specify something you don't want pickle here, remember to use copy to not modfiy orginal instance\"\n",
        "    state = self.__dict__.copy()\n",
        "    state['dataset'] = None\n",
        "    return state\n",
        "\n",
        "  #@delegates(TextDataloader.__init__) but we haven't evaluated TextDataloader\n",
        "  @delegates(TfmdDL.new)\n",
        "  @classmethod\n",
        "  def from_cache(cls, file_path, dataset, **kwargs):\n",
        "    dl = torch.load(file_path)\n",
        "    dl.dataset = dataset\n",
        "\n",
        "    # Reject change that cause arguments be inconsistent with loaded `self.samples` record \n",
        "    for arg in ['max_seq_len','agg_mode','ignore_gt_maxlen','remove_heads','remove_tails']:\n",
        "      assert arg not in kwargs, f\"Specifying {arg} will make it inconsistent with cached internal record.\"\n",
        "    if 'sort_by_len' in kwargs:\n",
        "      assert not (dl.sort_by_len and not kwargs['sort_by_len']), f\"Cached textdl is internal sorted, it can't restore orignal order.\"\n",
        "    for arg in ['bos_idx_add','eos_idx_add']:\n",
        "      if arg in kwargs: assert (kwargs[arg] is None) == (getattr(dl, arg) is None), f\"You can't change whether to add head/eos from cached setting.\"\n",
        "    # TextDataloader.new guess creating validation dataloader if don't drop_last, but it might not be the case\n",
        "    kwargs['ignore_gt_maxlen'] = dl.ignore_gt_maxlen\n",
        "    # Even if spefify no kwargs and just load original dataloader, using new method can update device and dtype of bos and eos for this dataset\n",
        "    dl = dl.new(dataset, samples=dl.samples, **kwargs)\n",
        "    # Consider whether setting up newly pased batch tfms, cuz new method just make do_setup=false\n",
        "    # Actually I don't know if it is good, but at leaat it works for pad_input_chunk as before_batch\n",
        "    if kwargs.pop('do_setup', True):\n",
        "      for nm in ['after_item','before_batch','after_batch']:\n",
        "        if nm in kwargs:\n",
        "          kwargs[nm] = Pipeline(kwargs.get(nm,None)) # don't know why Pipeline creating in TfmdDL won't be done in this case, but we can do it here and even it has done, it is ok we just Pipeline it again. \n",
        "          pv(f\"Setting up {nm}: {kwargs[nm]}\", kwargs.pop('verbose', False))\n",
        "          kwargs[nm].setup(dl)\n",
        "    return dl\n",
        "\n",
        "  @delegates(TfmdDL.new)\n",
        "  def new(self, dataset=None, **kwargs):\n",
        "    cur_args = dict(max_seq_len=self.max_seq_len, sort_by_len=self.sort_by_len,agg_mode=self.agg_mode,ignore_gt_maxlen=self.ignore_gt_maxlen,remove_heads=self.remove_heads, remove_tails=self.remove_tails, bos_idx_add=self.bos_idx_add, eos_idx_add=self.eos_idx_add,show_bar=self.show_bar)\n",
        "    \n",
        "    # we assume if you don't drop_last, you are going to create validation dl, specify ignore_gt_maxlen in kwargs to overwrite it if this is not in the case  \n",
        "    if not getattr(kwargs, 'drop_last', self.drop_last): \n",
        "      cur_args['ignore_gt_maxlen'] = False # You can't discard data from dataset for validation, especially test set\n",
        "    \n",
        "    return super().new(dataset=dataset,\n",
        "                       **merge(cur_args, kwargs)) # kwargs overwrite cur_args"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tchYkt4w5mDr",
        "colab_type": "text"
      },
      "source": [
        "# 2. Try"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCQiYfE4GIRk",
        "colab_type": "text"
      },
      "source": [
        "We'll try different param of `TextDataloader` to show its capability, but **!! it doesn't mean these are the best practices. !!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3m_Qjh97lgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db = DataBlock(splitter=ColSplitter(),\n",
        "              blocks=HF_TextBlock.from_df('text', hf_tokenizer),\n",
        "              get_x=ColReader('text'),)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylPTnYJk5o09",
        "colab_type": "text"
      },
      "source": [
        "Default behavior:\n",
        "* a line a sample\n",
        "* collect samples by their length. (try to make samples with the same length as a batch, to reduce number of pad)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oWfgKqJ5kjN",
        "colab_type": "code",
        "outputId": "198cf64f-4239-4ef4-c7a4-1b50e29bed46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        }
      },
      "source": [
        "default_dls = db.dataloaders(df, bs=4, dl_type=TextDataloader)\n",
        "default_dls.show_batch(max_n=4)\n",
        "\"\"\"\n",
        "We sort the sample by its length.\n",
        "Observe that the 3rd sample of batch is Friday (9 tokens) but not Thursday (10 tokens), \n",
        "thus we can reduce number of pad need to add, \n",
        "becuase we have to make all samples in a batch the same legth.\n",
        "\"\"\"\n",
        "print('x batch size:', default_dls.one_batch()[0].shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] i ' m burning like a fire gone wild on saturday [SEP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] guess i won ' t be coming to church on sunday [SEP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[CLS] thursday waiting for love , waiting for love [SEP] [PAD] [PAD] [PAD]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[CLS] thank the stars it ' s friday [SEP] [PAD] [PAD] [PAD] [PAD]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "x batch size: torch.Size([4, 13])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPbXr196wKks",
        "colab_type": "text"
      },
      "source": [
        "**Window mode**\n",
        "* Want to use broader context\n",
        "* sliding context window\n",
        "* less pad (only samples in the last batch may have pad)\n",
        "* every sample is of `max_seq_len` length. (Unless the last batch only have one sample shorter than `max_seq_len`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v-Zo8jTwKST",
        "colab_type": "code",
        "outputId": "840486fe-2176-45b2-9e77-3dbcf51fea15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "window_dls = db.dataloaders(df, shuffle_train=False, bs=2,\n",
        "                     dl_type=partial(TextDataloader,\n",
        "                                     max_seq_len=15,\n",
        "                                     agg_mode='window',\n",
        "                                     remove_heads=True,\n",
        "                                     remove_tails=True,\n",
        "                                     bos_idx_add=hf_tokenizer.cls_token_id,\n",
        "                                     eos_idx_add=hf_tokenizer.sep_token_id))\n",
        "window_dls.show_batch(max_n=2)\n",
        "\"\"\"\n",
        "To use CLS...SEP format, first remove heads(CLS) and tails(SEP) for every line,\n",
        "and then add bos(CLS) and eos(SEP) to the head and tail of concatenated sequence.\n",
        "\"\"\"\n",
        "print('x batch size:', window_dls.one_batch()[0].shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] monday left me broken tuesday i was through with hoping wednesday my empty [SEP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] arms were open thursday waiting for love , waiting for love thank the [SEP]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "x batch size: torch.Size([2, 15])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVIiXWnkrTWZ",
        "colab_type": "text"
      },
      "source": [
        "**Lines mode**\n",
        "* Want to attend to wider context, but also don't want shattered sentence.\n",
        "* Sequentially concat lines.\n",
        "* Note that `max_seq_len` is not definitely length of sample, and increasing it doesn't definitely increase number of pads used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r98xX2c3Ws9j",
        "colab_type": "code",
        "outputId": "4e0c6dca-4c35-4591-a404-b4ba37d69674",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "lines_dls = db.dataloaders(df, shuffle_train=False, bs=2,\n",
        "                     dl_type=partial(TextDataloader,\n",
        "                                     max_seq_len=13,\n",
        "                                     agg_mode='lines',\n",
        "                                     remove_heads=True,\n",
        "                                     bos_idx_add=hf_tokenizer.cls_token_id))\n",
        "lines_dls.show_batch(max_n=2)\n",
        "\"\"\"\n",
        "To get CLS ... SEP ... SEP format, we remove head (CLS) for every line,\n",
        "and add back an bos (CLS) to head of concated sample.\n",
        "\"\"\"\n",
        "print('x batch size:', lines_dls.one_batch()[0].shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] monday left me broken [SEP] tuesday i was through with hoping [SEP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] i ' m burning like a fire gone wild on saturday [SEP]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "x batch size: torch.Size([2, 13])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWHqTA6EzwjU",
        "colab_type": "text"
      },
      "source": [
        "**(Traditional) Language model mode**\n",
        "* predict i th token in y, using 0~i-1 tokens in x\n",
        "* sliding context window\n",
        "* samples in the last batch may have pad\n",
        "* every sample is of `max_seq_len` length. (Unless the last batch only have one sample shorter than `max_seq_len`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hijemiYrr9kV",
        "colab_type": "code",
        "outputId": "7b8c6980-d22d-4ee2-d8a2-c08fb25a7b81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "lm_dls = db.dataloaders(df, shuffle_train=False, bs=2,\n",
        "                        dl_type=partial(TextDataloader,\n",
        "                                        max_seq_len=7,\n",
        "                                        agg_mode='lm',))\n",
        "lm_dls.show_batch(max_n=2)\n",
        "print('x batch size:', lm_dls.one_batch()[0].shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] monday left me broken [SEP] [CLS]</td>\n",
              "      <td>monday left me broken [SEP] [CLS] tuesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i was through with hoping [SEP] [CLS]</td>\n",
              "      <td>was through with hoping [SEP] [CLS] wednesday</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "x batch size: torch.Size([2, 7])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XHQc_d4HKFS",
        "colab_type": "text"
      },
      "source": [
        "# 3. Speed comparison to existing dataloader for text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmiZeQRBKiyT",
        "colab_type": "text"
      },
      "source": [
        "Create datasets first to not count the time of creating datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeratlXOHnk1",
        "colab_type": "code",
        "outputId": "7631c254-8b06-4956-dd80-85e00db22107",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "another_df = load_demo_dataframe()\n",
        "print('Size of this demo dataset: ', len(another_df))\n",
        "another_datasets = db.datasets(another_df)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of this demo dataset:  14489\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfawXIM1adk_",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Compare time for initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAD-2rNmKQlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataloaders_from_db_and_datasets(db, dsets, path='.', verbose=False, **kwargs):\n",
        "    kwargs = {**db.dls_kwargs, **kwargs, 'verbose': verbose}\n",
        "    return dsets.dataloaders(path=path, after_item=db.item_tfms, after_batch=db.batch_tfms, **kwargs)\n",
        "get_dataloaders = partial(dataloaders_from_db_and_datasets, db, another_datasets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VynAPJE_IT_-",
        "colab_type": "code",
        "outputId": "f886fc31-7757-4e06-9106-d04380fe50b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%timeit get_dataloaders(dl_type=SortedDL)\n",
        "%timeit get_dataloaders(dl_type=partial(TextDataloader, sort_by_len='desc'))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 8.19 s per loop\n",
            "1 loop, best of 3: 7.25 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qot1RTQzO-8A",
        "colab_type": "code",
        "outputId": "b8527817-b137-4451-8fe1-4254f402fab3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%timeit get_dataloaders(dl_type=LMDataLoader)\n",
        "%timeit get_dataloaders(dl_type=partial(TextDataloader, max_seq_len=72, agg_mode='lm'))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 7.25 s per loop\n",
            "1 loop, best of 3: 7.27 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y425qGZVanSX",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 Compare time for load batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w5CVk5tPOoR",
        "colab_type": "text"
      },
      "source": [
        "Only `show_bar` is default to `dataset size > 200000`, to showcase bar feature, we force `show_bar` here "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8ximrGyasjU",
        "colab_type": "code",
        "outputId": "9c6dbb8e-2c2a-4cdb-cc2d-c8da9ea5d64e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# We reinitialize because assignment in %timeit is local to %timeit special function scope \n",
        "# BTW, there's four bar beacause there are two (train/valid) textdl for two dls each\n",
        "sorted_dls = get_dataloaders(dl_type=SortedDL)\n",
        "my_sorted_dls = get_dataloaders(dl_type=partial(TextDataloader, sort_by_len='desc', show_bar=True))\n",
        "LM_dls = get_dataloaders(dl_type=LMDataLoader)\n",
        "my_LM_dls = get_dataloaders(dl_type=partial(TextDataloader, max_seq_len=72, agg_mode='lm', show_bar=True))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TextDataloader init:: 100%|██████████| 6938/6938 [00:03<00:00, 1778.61it/s]\n",
            "TextDataloader init:: 100%|██████████| 7551/7551 [00:04<00:00, 1759.61it/s]\n",
            "TextDataloader init:: 100%|██████████| 6938/6938 [00:04<00:00, 1730.82it/s]\n",
            "TextDataloader init:: 100%|██████████| 7551/7551 [00:04<00:00, 1741.23it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_OWDBNab1Zw",
        "colab_type": "code",
        "outputId": "1a8e97a8-dbd8-4549-90c1-4e7786e4f0b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%timeit for b in sorted_dls.train: pass\n",
        "%timeit for b in my_sorted_dls.train: pass"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 5.31 s per loop\n",
            "1 loop, best of 3: 5.59 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4EwgBkXcHmx",
        "colab_type": "code",
        "outputId": "188b2857-324e-48ae-ac9d-0acd24d411dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%timeit for b in LM_dls.train: pass\n",
        "%timeit for b in my_LM_dls.train: pass"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 8.9 s per loop\n",
            "1 loop, best of 3: 5.93 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8Bw8VZWIMaX",
        "colab_type": "text"
      },
      "source": [
        "# 4. Cache\n",
        "So you don't need to initailize dataloader from scratch every time.\n",
        "\n",
        "Note that we cache mainly internal record of which sample should concatenate with which sample, but not the dataset itself. If you want cachable dataset, take a look at huggingface/nlp\n",
        "\n",
        "You should pass the same dataset, especially note that order of samples should be as the same as the original one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PslC2kEOtD2",
        "colab_type": "code",
        "outputId": "fdc3929b-3078-4fb2-9c55-6d0ac1de2a75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "same_datasets = db.datasets(df)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCRw7-lmk7Vg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataloaders_from_cache(db, source, file_paths, path='.', device=None, **kwargs):\n",
        "  device = default_device()\n",
        "  file_paths = L(file_paths).map(lambda p: Path(p))\n",
        "  datasets = db.datasets(source)\n",
        "  dl_s = L()\n",
        "  for i, f in enumerate(file_paths):\n",
        "    dl_s.append( TextDataloader.from_cache(f, datasets.subset(i), **kwargs) )\n",
        "  return DataLoaders(*dl_s, path=path, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OorV2Brov8--",
        "colab_type": "text"
      },
      "source": [
        "## 5.1 Cache and Loading cache"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7dQJcjvk9PZ",
        "colab_type": "code",
        "outputId": "578886b0-3643-4dd8-bb3f-033370fcbd28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "default_dls.train.cache('default_train.pth')\n",
        "loaded_default_dl = TextDataloader.from_cache('default_train.pth', same_datasets.subset(0))\n",
        "loaded_default_dl.show_batch()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] i ' m burning like a fire gone wild on saturday [SEP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] guess i won ' t be coming to church on sunday [SEP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[CLS] thursday waiting for love , waiting for love [SEP] [PAD] [PAD] [PAD]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[CLS] thank the stars it ' s friday [SEP] [PAD] [PAD] [PAD] [PAD]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Wp2n8fBPImf",
        "colab_type": "code",
        "outputId": "fe0deafd-98af-448c-8ccd-1a5766b3d00d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('Loaded:', TextDataloader.from_cache('default_train.pth', same_datasets.subset(0),\n",
        "                                        bs=3,pin_memory=True).one_batch()[0].shape )"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded: torch.Size([3, 13])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y5mqAqvvdwM",
        "colab_type": "code",
        "outputId": "6c279dea-3400-474c-f1ba-5b14d0dd09be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "window_dls.valid.cache('window_valid.pth')\n",
        "loaded_window_dl = TextDataloader.from_cache('window_valid.pth', same_datasets.subset(1))\n",
        "loaded_window_dl.show_batch()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] i ' ll be waiting for love , waiting for love to come [SEP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] around [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTicgZEmQ2ff",
        "colab_type": "text"
      },
      "source": [
        "Also can change args for `TextDataloader`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Asy3txnTPrWo",
        "colab_type": "code",
        "outputId": "26855f1b-bf7d-4c7e-d2c3-fbeb6b92649a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "TextDataloader.from_cache('window_valid.pth', same_datasets.subset(1), \n",
        "                          bos_idx_add=hf_tokenizer.unk_token_id).show_batch()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[UNK] i ' ll be waiting for love , waiting for love to come [SEP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[UNK] around [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSG8TFp-22mM",
        "colab_type": "text"
      },
      "source": [
        "To load dataloader's', you can simply refer to this script. \n",
        "\n",
        "Actually, to make `Dataloaders` from `dataloader`, all you need is pass the `dataloader` s and `path` and `device`. Easy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZZDRvAuvoUW",
        "colab_type": "code",
        "outputId": "0f748ddc-e9c5-4ddf-a647-95e90d8d7d57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "lines_dls.train.cache('lines_train.pth')\n",
        "lines_dls.valid.cache('lines_valid.pth')\n",
        "loaded_lines_dls = dataloaders_from_cache(db, df, ['lines_train.pth','lines_valid.pth'])\n",
        "loaded_lines_dls.show_batch()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] monday left me broken [SEP] tuesday i was through with hoping [SEP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] i ' m burning like a fire gone wild on saturday [SEP]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_4dTJFOP-ao",
        "colab_type": "code",
        "outputId": "5f1c1b35-d5a0-418e-e377-1dffbb9b1a56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "#bk()\n",
        "dataloaders_from_cache(db, df, ['lines_train.pth','lines_valid.pth'], \n",
        "                       before_batch=partial(pad_input_chunk, pad_first=False, \n",
        "                                            pad_idx=hf_tokenizer.unk_token_id)).show_batch()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] monday left me broken [SEP] tuesday i was through with hoping [SEP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] i ' m burning like a fire gone wild on saturday [SEP]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iah2CRiLvvb7",
        "colab_type": "code",
        "outputId": "dc4e93dc-5cab-4dc5-96a4-ae83bc4d4ca1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "lm_dls.train.cache('lm_train.pth')\n",
        "lm_dls.valid.cache('lm_valid.pth')\n",
        "loaded_lm_dls = dataloaders_from_cache(db, df, ['lm_train.pth','lm_valid.pth'])\n",
        "loaded_lm_dls.show_batch()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] monday left me broken [SEP] [CLS]</td>\n",
              "      <td>monday left me broken [SEP] [CLS] tuesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i was through with hoping [SEP] [CLS]</td>\n",
              "      <td>was through with hoping [SEP] [CLS] wednesday</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL4ZrRmUQXC7",
        "colab_type": "text"
      },
      "source": [
        "`TextDataloader` will also reject changes that make it inconsistent with loaded internal records.\n",
        "\n",
        "So you can be safe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUuwyhYkQEcr",
        "colab_type": "code",
        "outputId": "0d37d517-279f-4d9e-cf9b-5b976b523de8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "dataloaders_from_cache(db, df, ['lm_train.pth','lm_valid.pth'],  \n",
        "                       bos_idx_add=hf_tokenizer.unk_token_id).show_batch()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-772a52954fae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m dataloaders_from_cache(db, df, ['lm_train.pth','lm_valid.pth'],  \n\u001b[0;32m----> 2\u001b[0;31m                        bos_idx_add=hf_tokenizer.unk_token_id).show_batch()\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-7ff68ce5f3bb>\u001b[0m in \u001b[0;36mdataloaders_from_cache\u001b[0;34m(db, source, file_paths, path, device, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mdl_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdl_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mTextDataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataLoaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdl_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-2d7e5fdde274>\u001b[0m in \u001b[0;36mfrom_cache\u001b[0;34m(cls, file_path, dataset, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_by_len\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sort_by_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Cached textdl is internal sorted, it can't restore orignal order.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'bos_idx_add'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'eos_idx_add'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"You can't change whether to add head/eos from cached setting.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;31m# TextDataloader.new guess creating validation dataloader if don't drop_last, but it might not be the case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ignore_gt_maxlen'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_gt_maxlen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: You can't change whether to add head/eos from cached setting."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7urGBACF0q2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}