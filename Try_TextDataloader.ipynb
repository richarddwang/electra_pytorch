{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Try TextDataloader",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "777ffd8f40a9473ba5ee1ac4002d48bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_04fe940ec6c04f5aaadc09d8ef03f37e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2d1384b11b4f46e997e670baa86073a1",
              "IPY_MODEL_f70b060824f249a2979a72fff9cbacfe"
            ]
          }
        },
        "04fe940ec6c04f5aaadc09d8ef03f37e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2d1384b11b4f46e997e670baa86073a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_30524f12f264458284c035d1e5a5115a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1d691d1fba54b1c90616f8cf7cb43f6"
          }
        },
        "f70b060824f249a2979a72fff9cbacfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8efbb496a10d49d486cb2682f8fc1e30",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 397kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_739876edbd184c5db9d3f87959c003e0"
          }
        },
        "30524f12f264458284c035d1e5a5115a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1d691d1fba54b1c90616f8cf7cb43f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8efbb496a10d49d486cb2682f8fc1e30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "739876edbd184c5db9d3f87959c003e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9jAg3I25Kfr",
        "colab_type": "code",
        "outputId": "f5fa6770-170d-4f5c-c240-b71b4297170e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "!git clone https://github.com/richardyy1188/Pretrain-MLM-and-finetune-on-GLUE-with-fastai.git\n",
        "%cd Pretrain-MLM-and-finetune-on-GLUE-with-fastai\n",
        "%pip install -q fastai2 transformers h5py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Pretrain-MLM-and-finetune-on-GLUE-with-fastai'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 24 (delta 6), reused 20 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (24/24), done.\n",
            "/content/Pretrain-MLM-and-finetune-on-GLUE-with-fastai\n",
            "\u001b[K     |████████████████████████████████| 194kB 2.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 8.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 13.9MB/s \n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/sentencepiece/\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 32.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 21.3MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omRD3-nx5SVs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "777ffd8f40a9473ba5ee1ac4002d48bc",
            "04fe940ec6c04f5aaadc09d8ef03f37e",
            "2d1384b11b4f46e997e670baa86073a1",
            "f70b060824f249a2979a72fff9cbacfe",
            "30524f12f264458284c035d1e5a5115a",
            "e1d691d1fba54b1c90616f8cf7cb43f6",
            "8efbb496a10d49d486cb2682f8fc1e30",
            "739876edbd184c5db9d3f87959c003e0"
          ]
        },
        "outputId": "0c87cd6d-bb29-408f-d40f-f04e4110ce62"
      },
      "source": [
        "from IPython.core.debugger import set_trace as bk\n",
        "from functools import partial\n",
        "import h5py\n",
        "import torch\n",
        "from fastai2.text.all import *\n",
        "from transformers import ElectraTokenizer\n",
        "hf_tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-small-generator\")\n",
        "from _utils.hf_transformers_integration import HF_Tokenizer, HF_TextBlock, HFModelWrapper"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "777ffd8f40a9473ba5ee1ac4002d48bc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-SSKytH_6ns",
        "colab_type": "code",
        "outputId": "312668b5-7974-440d-b9d9-627a4f444351",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "source": [
        "# from the lyrics of \"Avicii - Waiting For Love\"\n",
        "df = pd.DataFrame(data={'text':[\"Monday left me broken\",\"Tuesday I was through with hoping\",\"Wednesday my empty arms were open\",\"Thursday waiting for love, waiting for love\",\"Thank the stars it's Friday\",\"I'm burning like a fire gone wild on Saturday\",\"Guess I won't be coming to church on Sunday\",\"I'll be waiting for love, waiting for love\",\"To come around\",],'is_valid':[False]*7 + [True]*2})\n",
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>is_valid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Monday left me broken</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tuesday I was through with hoping</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Wednesday my empty arms were open</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Thursday waiting for love, waiting for love</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thank the stars it's Friday</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>I'm burning like a fire gone wild on Saturday</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Guess I won't be coming to church on Sunday</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I'll be waiting for love, waiting for love</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>To come around</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            text  is_valid\n",
              "0                          Monday left me broken     False\n",
              "1              Tuesday I was through with hoping     False\n",
              "2              Wednesday my empty arms were open     False\n",
              "3    Thursday waiting for love, waiting for love     False\n",
              "4                    Thank the stars it's Friday     False\n",
              "5  I'm burning like a fire gone wild on Saturday     False\n",
              "6    Guess I won't be coming to church on Sunday     False\n",
              "7     I'll be waiting for love, waiting for love      True\n",
              "8                                 To come around      True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLiBRkiM5hgb",
        "colab_type": "text"
      },
      "source": [
        "#1. TextDataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xuPaHcu5jbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@delegates()\n",
        "class TextDataloader(TfmdDL):\n",
        "  def __init__(self, dataset, max_seq_len=float('inf'), sort_by_len=True, agg_mode=None, ignore_gt_maxlen=True, remove_heads=False, remove_tails=False, bos_idx_add=None, eos_idx_add=None, **kwargs):\n",
        "    super().__init__(dataset, **kwargs)\n",
        "    assert agg_mode in [None, 'lm', 'lines', 'window']\n",
        "    assert not (agg_mode and max_seq_len is None) \n",
        "    self.sort_by_len = sort_by_len and agg_mode in [None, 'lines'] # sorting makes sense only with these modes\n",
        "    ignore_gt_maxlen = ignore_gt_maxlen and agg_mode in [None, 'lines'] and max_seq_len is not None\n",
        "    first_text_tensor = dataset[0][0]\n",
        "    device, dtype = first_text_tensor.device, first_text_tensor.dtype\n",
        "    self.bos = torch.tensor([bos_idx_add] if bos_idx_add is not None else [], device=device, dtype=dtype)\n",
        "    self.eos = torch.tensor([eos_idx_add] if eos_idx_add is not None else [], device=device, dtype=dtype)\n",
        "\n",
        "    store_attr(self,'dataset,max_seq_len,sort_by_len,agg_mode,ignore_gt_maxlen,remove_heads,remove_tails')\n",
        "    \n",
        "    self.samples = L()\n",
        "    # residual_len will reset to initial_residual_len\n",
        "    # lm mode: max_seq_len text and 1 right-shift text, so take max_seq_len + 1 window\n",
        "    self.initial_residual_len = max_seq_len + 1 if agg_mode=='lm' else max_seq_len \n",
        "    # keep spaces to add bos to final text \n",
        "    if bos_idx_add is not None: self.initial_residual_len -= 1\n",
        "    if eos_idx_add is not None: self.initial_residual_len -= 1\n",
        "    self.residual_len, self.new_sample = self.initial_residual_len, []\n",
        "    # only use [start:end] text to concatenate (if needed)\n",
        "    self.start = 0 if not remove_heads else 1\n",
        "    self.end = None if not remove_tails else -1\n",
        "\n",
        "    for i, sample in enumerate(dataset):\n",
        "      line_len = len(sample[0])\n",
        "      if remove_heads: line_len -= 1\n",
        "      if remove_tails: line_len -= 1\n",
        "      \n",
        "      if max_seq_len is not None and line_len > self.initial_residual_len and agg_mode in [None, 'lines']:\n",
        "        if ignore_gt_maxlen: continue\n",
        "        else: raise ValueError(f'The {i} th text line in dataset has length {line_len}(without removing head or tail, {len(sample[0])}), and is longer than max length {self.initial_residual_len}(without add bos or eos, {max_seq_len})')\n",
        "        \n",
        "      if agg_mode is None: self.samples.append( (line_len, i) )\n",
        "      elif agg_mode == 'lines': self._accumulate_lines(i, line_len)\n",
        "      else: self._accumulate_window(i, line_len)\n",
        "\n",
        "    # specify total number of samples\n",
        "    self.n = len(self.samples)\n",
        "      \n",
        "  def _accumulate_lines(self, i, line_len):\n",
        "    if line_len <= self.residual_len:\n",
        "      self.new_sample.append(i)\n",
        "      self.residual_len -= line_len\n",
        "    else:\n",
        "      self.samples.append((self.max_seq_len-self.residual_len, self.new_sample))\n",
        "      self.new_sample = [i]\n",
        "      self.residual_len = self.initial_residual_len - line_len\n",
        "\n",
        "  def _accumulate_window(self, i, line_len):\n",
        "    usable_len = line_len\n",
        "    cursor = self.start\n",
        "    while usable_len != 0:\n",
        "      use_len = min(usable_len, self.residual_len)\n",
        "      self.new_sample.append((i, cursor, cursor+use_len))\n",
        "      self.residual_len -= use_len\n",
        "      usable_len -= use_len\n",
        "      cursor += use_len\n",
        "      if self.residual_len == 0:\n",
        "        self.samples.append(self.new_sample)\n",
        "        self.new_sample = []\n",
        "        self.residual_len = self.initial_residual_len\n",
        "\n",
        "  def create_item(self, s):\n",
        "    if self.agg_mode is None:\n",
        "      \"samples = [ (length, idx), ... ]\"\n",
        "      idx = self.samples[s][1]\n",
        "      sample = self.dataset[idx]\n",
        "      text= TensorText(torch.cat([self.bos, sample[0][self.start:self.end], self.eos]))\n",
        "      return ( text, *sample[1:] )\n",
        "    elif self.agg_mode == 'lines':\n",
        "      \"samples = [ (length, [idx, idx, ...]) , ... ]\"\n",
        "      agg_text = concat(*[ self.dataset[idx][0][self.start:self.end] for idx in self.samples[s][1] ] )\n",
        "      agg_text = TensorText(torch.cat([self.bos, agg_text, self.eos]))\n",
        "      return (agg_text, )\n",
        "    else: # window or lm\n",
        "      \"samples = [ (idx,start,end) ]\"\n",
        "      agg_text = concat(*[ self.dataset[idx][0][start:end] for idx,start,end in self.samples[s] ])\n",
        "      agg_text = TensorText(torch.cat([self.bos, agg_text, self.eos]))\n",
        "      if self.agg_mode == 'window':\n",
        "        return (agg_text, )\n",
        "      else: # 'lm'\n",
        "        return (LMTensorText(agg_text[:-1]), agg_text[1:])\n",
        "\n",
        "  def shuffle_fn(self, idxs):\n",
        "    if self.agg_mode in ['lm', 'window']:\n",
        "      self.samples.shuffle()\n",
        "    else:\n",
        "      self.samples.sort(key=lambda s: s[0])\n",
        "    return idxs\n",
        "\n",
        "  @delegates(TfmdDL.new)\n",
        "  def new(self, dataset=None, **kwargs):\n",
        "    # assume val data should use the same max_seq_len with train data, single_line\n",
        "    'dataset,max_seq_len,sort_by_len,agg_mode,concat,ignore_gt_maxlen'\n",
        "    return TextDataloader(dataset=dataset,\n",
        "                          # if lm, pass max_seq_len after restore to original value\n",
        "                          max_seq_len=self.max_seq_len,\n",
        "                          sort_by_len=False, # valid set even don't shuffle\n",
        "                          agg_mode=self.agg_mode,\n",
        "                          ignore_gt_maxlen=False, # You can't discard data from valid set, especially test set\n",
        "                          **kwargs,\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ade5E5dF-0Rr",
        "colab_type": "text"
      },
      "source": [
        "# 2. Valid dataloader problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl83PzNvH5TL",
        "colab_type": "text"
      },
      "source": [
        "`Filteredbase.dataloaders` didn't merge kwargs When creating validation dataloader, \n",
        "which makes validation dataloader don't have `pad_input_chunk` as before_batch transform,\n",
        "that specified in `Textblock.__init__`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPuODNl8-xdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mydataloaders(self, bs=64, val_bs=None, shuffle_train=True, n=None, path='.', dl_type=None, dl_kwargs=None,\n",
        "                    device=None, **kwargs):\n",
        "  if device is None: device=default_device()\n",
        "  if dl_kwargs is None: dl_kwargs = [{}] * self.n_subsets\n",
        "  if dl_type is None: dl_type = self._dl_type\n",
        "  drop_last = kwargs.pop('drop_last', shuffle_train)\n",
        "  dl = dl_type(self.subset(0), bs=bs, shuffle=shuffle_train, drop_last=drop_last, n=n, device=device,\n",
        "               **merge(kwargs, dl_kwargs[0]))\n",
        "  dls = [dl] + [dl.new(self.subset(i), bs=(bs if val_bs is None else val_bs), shuffle=False, drop_last=False,\n",
        "                       n=None, **merge(kwargs, dl_kwargs[i])) for i in range(1, self.n_subsets)]\n",
        "  return self._dbunch_type(*dls, path=path, device=device)\n",
        "\n",
        "class MyDataBlock(DataBlock):\n",
        "  def dataloaders(self, source, path='.', verbose=False, **kwargs):\n",
        "    dsets = self.datasets(source)\n",
        "    kwargs = {**self.dls_kwargs, **kwargs, 'verbose': verbose}\n",
        "    return mydataloaders(dsets, path=path, after_item=self.item_tfms, after_batch=self.batch_tfms, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tchYkt4w5mDr",
        "colab_type": "text"
      },
      "source": [
        "# 3. Try"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCQiYfE4GIRk",
        "colab_type": "text"
      },
      "source": [
        "We'll try different param of `TextDataloader` to show its capability, but **!! it doesn't mean these are the best practices. !!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3m_Qjh97lgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db = DataBlock(splitter=ColSplitter(),\n",
        "              blocks=HF_TextBlock.from_df('text', hf_tokenizer),\n",
        "              get_x=ColReader('text'),)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylPTnYJk5o09",
        "colab_type": "text"
      },
      "source": [
        "Default behavior:\n",
        "* a line a sample\n",
        "* collect samples by their length. (try to make samples with the same length as a batch, to reduce number of pad)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oWfgKqJ5kjN",
        "colab_type": "code",
        "outputId": "4580b204-0777-4b10-9485-8d4111f2207c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        }
      },
      "source": [
        "dls = db.dataloaders(df, bs=4, shuffle_train=True, pad_first=False, dl_type=TextDataloader)\n",
        "dls.show_batch(max_n=4)\n",
        "\"\"\"\n",
        "We sort the sample by its length.\n",
        "Observe that the 3rd sample of batch is Friday (9 tokens) but not Thursday (10 tokens), \n",
        "thus we can reduce number of pad need to add, \n",
        "becuase we have to make all samples in a batch the same legth.\n",
        "\"\"\"\n",
        "print('x batch size:', dls.one_batch()[0].shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] monday left me broken [SEP] [PAD] [PAD] [PAD]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] tuesday i was through with hoping [SEP] [PAD]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[CLS] wednesday my empty arms were open [SEP] [PAD]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[CLS] thank the stars it ' s friday [SEP]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "x batch size: torch.Size([4, 9])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPbXr196wKks",
        "colab_type": "text"
      },
      "source": [
        "**Window mode**\n",
        "* Want to use broader context\n",
        "* sliding context window\n",
        "* less pad (only samples in the last batch may have pad)\n",
        "* every sample is of `max_seq_len` length. (Unless the last batch only have one sample shorter than `max_seq_len`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v-Zo8jTwKST",
        "colab_type": "code",
        "outputId": "ca4078ac-6292-4721-a6a4-0a9928f121e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "dls = db.dataloaders(df, shuffle_train=False, pad_first=False, bs=2,\n",
        "                     dl_type=partial(TextDataloader,\n",
        "                                     max_seq_len=15,\n",
        "                                     agg_mode='window',\n",
        "                                     remove_heads=True,\n",
        "                                     remove_tails=True,\n",
        "                                     bos_idx_add=hf_tokenizer.cls_token_id,\n",
        "                                     eos_idx_add=hf_tokenizer.sep_token_id))\n",
        "dls.show_batch(max_n=2)\n",
        "\"\"\"\n",
        "To use CLS...SEP format, first remove heads(CLS) and tails(SEP) for every line,\n",
        "and then add bos(CLS) and eos(SEP) to the head and tail of concatenated sequence.\n",
        "\"\"\"\n",
        "print('x batch size:', dls.one_batch()[0].shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] monday left me broken tuesday i was through with hoping wednesday my empty [SEP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] arms were open thursday waiting for love , waiting for love thank the [SEP]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "x batch size: torch.Size([2, 15])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVIiXWnkrTWZ",
        "colab_type": "text"
      },
      "source": [
        "**Lines mode**\n",
        "* Want to attend to wider context, but also don't want shattered sentence.\n",
        "* Sequentially concat lines.\n",
        "* Note that `max_seq_len` is not definitely length of sample, and increasing it doesn't definitely increase number of pads used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r98xX2c3Ws9j",
        "colab_type": "code",
        "outputId": "1adab33c-fe4f-4117-8bfd-ba7b5f334703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "dls = db.dataloaders(df, shuffle_train=False, pad_first=False, bs=2,\n",
        "                     dl_type=partial(TextDataloader,\n",
        "                                     max_seq_len=13,\n",
        "                                     agg_mode='lines',\n",
        "                                     remove_heads=True,\n",
        "                                     bos_idx_add=hf_tokenizer.cls_token_id))\n",
        "dls.show_batch(max_n=2)\n",
        "\"\"\"\n",
        "To get CLS ... SEP ... SEP format, we remove head (CLS) for every line,\n",
        "and add back an bos (CLS) to head of concated sample.\n",
        "\"\"\"\n",
        "print('x batch size:', dls.one_batch()[0].shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] monday left me broken [SEP] tuesday i was through with hoping [SEP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] wednesday my empty arms were open [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "x batch size: torch.Size([2, 13])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWHqTA6EzwjU",
        "colab_type": "text"
      },
      "source": [
        "**(Traditional) Language model mode**\n",
        "* predict i th token in y, using 0~i-1 tokens in x\n",
        "* sliding context window\n",
        "* samples in the last batch may have pad\n",
        "* every sample is of `max_seq_len` length. (Unless the last batch only have one sample shorter than `max_seq_len`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hijemiYrr9kV",
        "colab_type": "code",
        "outputId": "815a57ac-4532-4b02-b17c-8854deec2acd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "dls = db.dataloaders(df, shuffle_train=False, pad_first=False, bs=2,\n",
        "                     dl_type=partial(TextDataloader,\n",
        "                                     max_seq_len=7,\n",
        "                                     agg_mode='lm',))\n",
        "dls.show_batch(max_n=2)\n",
        "print('x batch size:', dls.one_batch()[0].shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] monday left me broken [SEP] [CLS]</td>\n",
              "      <td>monday left me broken [SEP] [CLS] tuesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i was through with hoping [SEP] [CLS]</td>\n",
              "      <td>was through with hoping [SEP] [CLS] wednesday</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "x batch size: torch.Size([2, 7])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8Bw8VZWIMaX",
        "colab_type": "text"
      },
      "source": [
        "# 4. Cache (developing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i5w6eJoIPM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def export_data(dataloaders, file):\n",
        "  file = Path(file).resolve()\n",
        "  if file.exists(): print('Append the data into the file. Note that this blends it with existing data in the file. ')\n",
        "  for i, dl in enumerate(dataloaders):\n",
        "    export_dataset(dl.dataset, file, i)\n",
        "\n",
        "def export_dataset(dataset, file, dataset_idx=0):\n",
        "  # h5 datasets will store variable 1d array of dtypes respectively\n",
        "  dtypes = [ h5py.special_dtype(vlen=t.numpy().dtype)  for t in dataset[0] ] \n",
        "  \n",
        "  with h5py.File(file, mode='a') as f:\n",
        "    \n",
        "    # f contains datasets (h5groups), first dataset will be train dataset\n",
        "    if f'dataset_{dataset_idx}' in f.keys():\n",
        "      ds = f[f'dataset_{dataset_idx}']\n",
        "    else:\n",
        "      ds = f.create_group(f'dataset_{dataset_idx}')\n",
        "\n",
        "    # dataset contains data slices (h5groups), i th data slice is i th export of data\n",
        "    # last character in the name of last group\n",
        "    slice_idx = int(list(f.keys())[-1][-1]) + 1 if f.keys() else 0\n",
        "    g = ds.create_group(f'slice_{slice_idx}')\n",
        "    \n",
        "    # data slice contain data elements (h5datasets), first data element will be Learner.xb[0]\n",
        "    try:\n",
        "      for i, (tl, dt) in enumerate(zip(dataset.tls, dtypes)):\n",
        "        # data elment i contains samples[:][i] (1d numpy arrays)\n",
        "        h5ds = g.create_dataset(name=str(i), shape=(len(dataset),), dtype=dt)\n",
        "        h5ds[:] = tl[:]\n",
        "    except Exception as e:\n",
        "      del f[f'slice_{slice_idx}']\n",
        "      raise e"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHXSVNJt_gVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DatasetsByH5(FilteredBase):\n",
        "  def __init__(self, h5_file, decode_funcs=None, load_to_memory=True):\n",
        "    self.f = h5py.File(h5_file, mode='r', driver='core' if load_to_memory else None)\n",
        "    self.decode_funcs = decode_funcs\n",
        "\n",
        "  @property\n",
        "  def n_subsets(self): return len(self.f)\n",
        "\n",
        "  def subset(self, i):\n",
        "    return DatasetByH5(self.f, list(self.f[f'dataset_{i}'].values()), decode_funcs=self.decode_funcs)\n",
        "  \n",
        "  def close(self): self.f.close()\n",
        "\n",
        "def _slice_num_samples(data_slice): return len(data_slice['0']) # length of first data element\n",
        "\n",
        "class DatasetByH5():\n",
        "  def __init__(self, h5f, h5groups, decode_funcs=None):\n",
        "    self.f = h5f\n",
        "    self.slices = h5groups\n",
        "    self.decode_funcs = decode_funcs if isinstance(decode_funcs, (list,tuple)) or decode_funcs is None else [decode_funcs]\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    for data_slice in self.slices:\n",
        "      if i < _slice_num_samples(data_slice):\n",
        "        # group is {'0':h5ds, '1':h5ds,...,'num_samples':int}  \n",
        "        return tuple( torch.from_numpy(h5ds[i]) for h5ds in data_slice.values() ) \n",
        "      else:\n",
        "        i -= _slice_num_samples(data_slice)\n",
        "\n",
        "  def __len__(self):\n",
        "    l = 0\n",
        "    for data_slice in self.slices: l += _slice_num_samples(data_slice)\n",
        "    return l\n",
        "\n",
        "  def decode(self, sample, full=True): # full has not effect here\n",
        "    assert self.decode_funcs is not None, 'No decode function passed.'\n",
        "    return tuple( dec_fc(e) for e, dec_fc in zip(sample, self.decode_funcs))\n",
        "\n",
        "  def close(self): self.f.close() # Note it may close other DatasetByH5 because of shared h5file\n",
        "\n",
        "@delegates(FilteredBase.dataloaders)\n",
        "def import_data(h5_file, load_to_memory=True, decode_funcs=None, **kwargs):\n",
        "  dsets = DatasetsByH5(h5_file, load_to_memory=load_to_memory, decode_funcs=decode_funcs)\n",
        "  try:\n",
        "    return dsets.dataloaders(**kwargs)\n",
        "  except Exception as e:\n",
        "    dsets.close()\n",
        "    raise e"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7Lp8CqDvhrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "export_data(dls, 'test_data.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PudpprlAFvG2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "b7375a49-1215-4a88-d5c4-0baad101d6b2"
      },
      "source": [
        "dlss = import_data('test_data.h5', bs=2,\n",
        "                   dl_type=SortedDL,\n",
        "                   dls_kwargs={'before_batch': partial(pad_input_chunk,\n",
        "                                                       pad_idx=hf_tokenizer.pad_token_id,\n",
        "                                                       pad_first=False)},\n",
        "                   decode_funcs=[compose(hf_tokenizer.convert_ids_to_tokens, ' '.join ),]\n",
        "                   )\n",
        "dls.show_batch()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] monday left me broken [SEP] [CLS]</td>\n",
              "      <td>monday left me broken [SEP] [CLS] tuesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i was through with hoping [SEP] [CLS]</td>\n",
              "      <td>was through with hoping [SEP] [CLS] wednesday</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZwBeyf5IPUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dlss.train_ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8XGzrQiEYCL",
        "colab_type": "code",
        "outputId": "bdf36af9-8ad3-4a35-fdf9-e447d4303d54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        }
      },
      "source": [
        "dlss = import_data('test_data.h5', bs=2,\n",
        "                   dl_type=partial(TextDataloader,\n",
        "                                     max_seq_len=15,\n",
        "                                     agg_mode='window',\n",
        "                                     remove_heads=True,\n",
        "                                     remove_tails=True,\n",
        "                                     bos_idx_add=hf_tokenizer.cls_token_id,\n",
        "                                     eos_idx_add=hf_tokenizer.sep_token_id),\n",
        "                   dls_kwargs={'before_batch': partial(pad_input_chunk,\n",
        "                                                       pad_idx=hf_tokenizer.pad_token_id,\n",
        "                                                       pad_first=False)},\n",
        "                   decode_funcs=[compose(hf_tokenizer.convert_ids_to_tokens, ' '.join ),]\n",
        "                   )\n",
        "dls.show_batch()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ca7e719bd601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                        \u001b[0mpad_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                        pad_first=False)},\n\u001b[0;32m---> 12\u001b[0;31m                    \u001b[0mdecode_funcs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                    )\n\u001b[1;32m     14\u001b[0m \u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-d362d2082720>\u001b[0m in \u001b[0;36mimport_data\u001b[0;34m(h5_file, load_to_memory, decode_funcs, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mdsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-d362d2082720>\u001b[0m in \u001b[0;36mimport_data\u001b[0;34m(h5_file, load_to_memory, decode_funcs, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0mdsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetsByH5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_to_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_to_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_funcs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_funcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mdsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai2/data/core.py\u001b[0m in \u001b[0;36mdataloaders\u001b[0;34m(self, bs, val_bs, shuffle_train, n, path, dl_type, dl_kwargs, device, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mdrop_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drop_last'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         dl = dl_type(self.subset(0), bs=bs, shuffle=shuffle_train, drop_last=drop_last, n=n, device=device,\n\u001b[0;32m--> 201\u001b[0;31m                      **merge(kwargs, dl_kwargs[0]))\n\u001b[0m\u001b[1;32m    202\u001b[0m         dls = [dl] + [dl.new(self.subset(i), bs=(bs if val_bs is None else val_bs), shuffle=False, drop_last=False,\n\u001b[1;32m    203\u001b[0m                              n=None, **dl_kwargs[i]) for i in range(1, self.n_subsets)]\n",
            "\u001b[0;32m<ipython-input-4-61fa0a353d3c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, max_seq_len, sort_by_len, agg_mode, ignore_gt_maxlen, remove_heads, remove_tails, bos_idx_add, eos_idx_add, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m       \u001b[0mline_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mremove_heads\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline_len\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mremove_tails\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline_len\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHpyHAhEm1UL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dlss = import_data('test_data.h5', bs=2,\n",
        "                   dl_type=partial(TextDataloader,\n",
        "                                     max_seq_len=13,\n",
        "                                     agg_mode='lines',\n",
        "                                     remove_heads=True,\n",
        "                                     bos_idx_add=hf_tokenizer.cls_token_id),\n",
        "                   dls_kwargs={'before_batch': partial(pad_input_chunk,\n",
        "                                                       pad_idx=hf_tokenizer.pad_token_id,\n",
        "                                                       pad_first=False)},\n",
        "                   decode_funcs=[compose(hf_tokenizer.convert_ids_to_tokens, ' '.join ),]\n",
        "                   )\n",
        "dls.show_batch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfs41-dGHrww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dlss = import_data('test_data.h5', bs=2,\n",
        "                   dl_type=partial(TextDataloader,\n",
        "                                     max_seq_len=7,\n",
        "                                     agg_mode='lm',)\n",
        "                   dls_kwargs={'before_batch': partial(pad_input_chunk,\n",
        "                                                       pad_idx=hf_tokenizer.pad_token_id,\n",
        "                                                       pad_first=False)},\n",
        "                   decode_funcs=[compose(hf_tokenizer.convert_ids_to_tokens, ' '.join ),]\n",
        "                   )\n",
        "dls.show_batch()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}